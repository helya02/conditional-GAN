{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6gRuVJfq6ic"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle\n",
        "! mkdir ~/.kaggle\n",
        "#Upload the token json file\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO7bqbg0qsOr"
      },
      "outputs": [],
      "source": [
        "! kaggle datasets download -d shravankumar9892/image-colorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_6qHZjTrJFx"
      },
      "outputs": [],
      "source": [
        "!unzip \"image-colorization.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oFUXfbHtRaX"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Import glob to get the files directories recursively\n",
        "import glob\n",
        "\n",
        "# Import Garbage collector interface\n",
        "import gc\n",
        "\n",
        "# Import OpenCV to transforme pictures\n",
        "import cv2\n",
        "\n",
        "# Import Time\n",
        "import time\n",
        "\n",
        "# import numpy for math calculations\n",
        "import numpy as np\n",
        "\n",
        "# Import pandas for data (csv) manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Import matplotlib for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.style.use('fivethirtyeight')\n",
        "%matplotlib inline\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "\n",
        "#import pytorch_lightning as pl\n",
        "\n",
        "# Import pytorch to build Deel Learling Models\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "from torch.nn import functional as F\n",
        "import torch.utils.data\n",
        "from torchvision.models.inception import inception_v3\n",
        "from scipy.stats import entropy\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "# Import tqdm to show a smart progress meter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Import warnings to hide the unnessairy warniings\n",
        "import torchvision.utils as vutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYrsMP3_tJCv"
      },
      "outputs": [],
      "source": [
        "ab_path = \"/content/ab/ab/ab1.npy\"\n",
        "l_path = \"/content/l/gray_scale.npy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gunzONo6tJq9"
      },
      "outputs": [],
      "source": [
        "ab_df = np.load(ab_path)[0:100]\n",
        "L_df = np.load(l_path)[0:100]\n",
        "dataset = (L_df,ab_df)\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhmG7mR_rG_L"
      },
      "outputs": [],
      "source": [
        "def setup_input(self, data):\n",
        "        self.L = data['L']\n",
        "        self.ab = data['ab']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngf = 64"
      ],
      "metadata": {
        "id": "5I2ta6Xn7ge8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHx8FtrceWEm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "\n",
        "\n",
        "'''class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)'''\n",
        "\n",
        "'''class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.inc = ConvNet(2, 64)\n",
        "        self.down1 = DownConv(64, 128)\n",
        "        self.down2 = DownConv(128, 256)\n",
        "        self.down3 = DownConv(256, 512)\n",
        "        self.down4 = DownConv(512, 1024)\n",
        "\n",
        "        self.up1 = UpConv(1024, 512)\n",
        "        self.up2 = UpConv(512, 256)\n",
        "        self.up3 = UpConv(256, 128)\n",
        "        self.up4 = UpConv(128, 64)\n",
        "        self.outc = nn.Conv2d(64, 2 , 1)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = torch.cat([x, y], 1)\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        # print(\"Finished Encoder ------------\")\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits'''\n",
        "'''class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.down1 = DoubleConv(2, 64)\n",
        "        self.down2 = DoubleConv(64, 128)\n",
        "        self.down3 = DoubleConv(128, 256)\n",
        "        self.down4 = DoubleConv(256, 512)\n",
        "        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.up4 = nn.ConvTranspose2d(64, 2, kernel_size=2, stride=2)\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = torch.cat([x, y], 1)\n",
        "        x1 = self.down1(x)\n",
        "        x2 = self.max_pool(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.max_pool(x3)\n",
        "        x5 = self.down3(x4)\n",
        "        x6 = self.max_pool(x5)\n",
        "        x7 = self.down4(x6)\n",
        "        x = self.up1(x7)\n",
        "        x = torch.cat([x, x5], dim=1)\n",
        "        x = self.up2(x)\n",
        "        x = torch.cat([x, x3], dim=1)\n",
        "        x = self.up3(x)\n",
        "        x = torch.cat([x, x1], dim=1)\n",
        "        x = self.up4(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x'''\n",
        "\n",
        "\n",
        "'''class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(2, 64, 4, stride=1, padding=0, bias=False)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 4, stride=2, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 4, stride=2, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, 4, stride=2, padding=1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.conv5 = nn.Conv2d(512, 512, 4, stride=2, padding=1, bias=False)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "        self.conv6 = nn.Conv2d(512, 512, 4, stride=2, padding=1, bias=False)\n",
        "        self.bn6 = nn.BatchNorm2d(512)\n",
        "        self.conv7 = nn.Conv2d(512, 512, 4, stride=2, padding=1, bias=False)\n",
        "        self.bn7 = nn.BatchNorm2d(512)\n",
        "        self.conv8 = nn.Conv2d(512, 512, 4, stride=2, padding=1, bias=False)\n",
        "        self.bn8 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.deconv1 = nn.ConvTranspose2d(512, 512, 4, stride=2, padding=1, bias=False)\n",
        "        self.bn9 = nn.BatchNorm2d(512)\n",
        "        self.deconv2 = nn.ConvTranspose2d(1024, 512, 4, stride=2, padding=1, bias=False)\n",
        "        self.bn10 = nn.BatchNorm2d(512)\n",
        "        self.deconv3 = nn.ConvTranspose2d(1024, 512, 4, stride=2, padding=1, bias=False)\n",
        "        self.bn11 = nn.BatchNorm2d(512)\n",
        "        self.deconv4 = nn.ConvTranspose2d(1024, 512, 4, stride=2, padding=1, bias=False)\n",
        "        self.bn12 = nn.BatchNorm2d(512)\n",
        "        self.deconv5 = nn.ConvTranspose2d(1024, 256, 4, stride=2, padding=1, bias=False)\n",
        "        self.bn13 = nn.BatchNorm2d(256)\n",
        "        self.deconv6 = nn.ConvTranspose2d(512, 128, 4, stride=2, padding=1, bias=False)\n",
        "        self.bn14 = nn.BatchNorm2d(128)\n",
        "        self.deconv7 = nn.ConvTranspose2d(256, 64, 4, stride=2, padding=1, bias=False)\n",
        "        self.bn15 = nn.BatchNorm2d(64)\n",
        "        self.deconv8 = nn.ConvTranspose2d(128, 2, 4, stride=2, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Concatenate the input image and the condition image\n",
        "        x = torch.cat([x, y], 1)\n",
        "\n",
        "        # Encoder\n",
        "        x1 = nn.functional.relu(self.conv1(x))\n",
        "        x2 = nn.functional.relu(self.bn2(self.conv2(x1)))\n",
        "        x3 = nn.functional.relu(self.bn3(self.conv3(x2)))\n",
        "        x4 = nn.functional.relu(self.bn4(self.conv4(x3)))\n",
        "        x5 = nn.functional.relu(self.bn5(self.conv5(x4)))\n",
        "        x6 = nn.functional.relu(self.bn6(self.conv6(x5)))\n",
        "        x7 = nn.functional.relu(self.bn7(self.conv7(x6)))\n",
        "        x8 = nn.functional.relu(self.bn8(self.conv8(x7)))\n",
        "\n",
        "        # Decoder\n",
        "        y1 = nn.functional.relu(self.bn9(self.deconv1(x8)))\n",
        "        y2 = nn.functional.relu(self.bn10(self.deconv2(torch.cat([y1, x7], 1))))\n",
        "        y3 = nn.functional.relu(self.bn11(self.deconv3(torch.cat([y2, x6], 1))))\n",
        "        y4 = nn.functional.relu(self.bn12(self.deconv4(torch.cat([y3, x5], 1))))\n",
        "        y5 = nn.functional.relu(self.bn13(self.deconv5(torch.cat([y4, x4], 1))))\n",
        "        y6 = nn.functional.relu(self.bn14(self.deconv6(torch.cat([y5, x3], 1))))\n",
        "        y7 = nn.functional.relu(self.bn15(self.deconv7(torch.cat([y6, x2], 1))))\n",
        "        y8 = self.deconv8(torch.cat([y7, x1], 1))\n",
        "\n",
        "        # Output\n",
        "        return torch.tanh(y8)'''\n",
        "\n",
        "\n",
        "'''class DCGAN_generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DCGAN_generator, self).__init__()\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "\n",
        "            nn.ConvTranspose2d(2, ngf * 16, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 16),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*16) x 4 x 4\n",
        "            nn.ConvTranspose2d( ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 16 x 16\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 32 x 32\n",
        "            nn.ConvTranspose2d(2 * ngf, 3, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        input = torch.cat([x, y], 1)\n",
        "        return self.main(input)'''\n",
        "\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Load the pre-trained ResNet backbone\n",
        "        backbone = models.resnet34(pretrained=True)\n",
        "        self.label_emb = nn.Embedding(3, 3)\n",
        "\n",
        "        # Modify the first layer to accept the desired input channel size\n",
        "        backbone.conv1 = nn.Conv2d(2, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "        # Retrieve the encoder and decoder blocks from the backbone\n",
        "        self.encoder = nn.Sequential(*list(backbone.children())[:-2])\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
        "            nn.ConvTranspose2d(64, 2, kernel_size=4, stride=4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Perform forward pass through the UNet model\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the custom UNet model\n",
        "in_channels = 2  # Change the input channel size here\n",
        "num_classes = 3  # Change the number of output classes here\n",
        "u = UNet()\n",
        "input_tensor = torch.randn(1, 2, 224, 224)  # Replace with your own input tensor shape\n",
        "\n",
        "# Forward pass through the model\n",
        "output = u(input_tensor)\n",
        "\n",
        "# Print the output shape\n",
        "print(output.shape)\n",
        "\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, 5, stride=1, padding=2, bias=False)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 5, stride=1, padding=2, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 5, stride=1, padding=2, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, 5, stride=1, padding=2, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.conv5 = nn.Conv2d(512, 3, 5, stride=1, padding=2, bias=False)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        #x -> L\n",
        "        #y -> generator -ab\n",
        "        #z -> real_image\n",
        "        # Concatenate the input image and the condition image\n",
        "        #y = self.label_emb(y)\n",
        "        #x = x.view(-1, 224)\n",
        "        x_ = torch.cat([x, y], 1)\n",
        "\n",
        "        # Forward pass\n",
        "        x1 = nn.functional.leaky_relu(self.conv1(x_), 0.2)\n",
        "        x2 = nn.functional.leaky_relu(self.bn2(self.conv2(x1)), 0.2)\n",
        "        x3 = nn.functional.leaky_relu(self.bn3(self.conv3(x2)), 0.2)\n",
        "        x4 = nn.functional.leaky_relu(self.bn4(self.conv4(x3)), 0.2)\n",
        "        x5 = self.conv5(x4)\n",
        "        out = torch.sigmoid(x5)\n",
        "\n",
        "        # Output\n",
        "        return out\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clTQYYj-xmSy"
      },
      "outputs": [],
      "source": [
        "class ImageColorizationDataset(Dataset):\n",
        "    ''' Black and White (L) Images and corresponding A&B Colors'''\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        '''\n",
        "        :param dataset: Dataset name.\n",
        "        :param data_dir: Directory with all the images.\n",
        "        :param transform: Optional transform to be applied on sample\n",
        "        '''\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset[0])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        L = np.array(dataset[0][idx]).reshape((224,224,1))\n",
        "        L = transforms.ToTensor()(L)\n",
        "\n",
        "        ab = np.array(dataset[1][idx])\n",
        "        ab = transforms.ToTensor()(ab)\n",
        "\n",
        "        return L, ab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "KD8CCiiN915i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEwwVPVUtMJk"
      },
      "outputs": [],
      "source": [
        "def lab_to_rgb(L, ab):\n",
        "    \"\"\"\n",
        "    Takes an image or a batch of images and converts from LAB space to RGB\n",
        "    \"\"\"\n",
        "    L = L  * 100\n",
        "    ab = (ab - 0.5) * 128 * 2\n",
        "    Lab = torch.cat([L, ab], dim=1).numpy()\n",
        "    rgb_imgs = []\n",
        "    for img in Lab:\n",
        "        img_rgb = lab2rgb(img)\n",
        "        rgb_imgs.append(img_rgb)\n",
        "    return np.stack(rgb_imgs, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06cWEVC2pdn_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"max_split_size_mb:512\"  # Set the desired value for max_split_size_mb\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the loss functions and optimizers\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "generator = UNet().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Load the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(64),\n",
        "    transforms.CenterCrop(64),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = ImageColorizationDataset(dataset)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=1, shuffle = True, pin_memory = True)\n",
        "img_list = []\n",
        "# Train the generator and discriminator\n",
        "for epoch in range(100):\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        # Generate fake images\n",
        "\n",
        "\n",
        "\n",
        "        #fixed_noise = torch.randn(x.size(0),1, 224, 224).to(device)\n",
        "\n",
        "        input = torch.cat([x, torch.randn(x.size(0),1, 224, 224).to(device)], 1)\n",
        "\n",
        "\n",
        "        y_fake = generator(input)\n",
        "        # Train the discriminator\n",
        "        optimizer_d.zero_grad()\n",
        "\n",
        "\n",
        "        y_real = torch.ones(x.size(0), 3, 224, 224).to(device)\n",
        "\n",
        "        y_fake_ = torch.zeros(x.size(0), 3, 224, 224).to(device)\n",
        "\n",
        "\n",
        "\n",
        "        d_real = discriminator(x, y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        d_fake = discriminator(x, y_fake)\n",
        "\n",
        "        loss_d = criterion(d_real, y_real) + criterion(d_fake, y_fake_)\n",
        "        loss_d.backward()\n",
        "\n",
        "\n",
        "\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Train the generator\n",
        "        optimizer_g.zero_grad()\n",
        "        input_1 = torch.cat([x, torch.randn(x.size(0),1, 224, 224).to(device)], 1)\n",
        "        y_fake = generator(input_1)\n",
        "\n",
        "        d_fake = discriminator(x, y_fake)\n",
        "\n",
        "\n",
        "\n",
        "        loss_g = criterion(d_fake, y_real)\n",
        "        loss_g.backward()\n",
        "\n",
        "        optimizer_g.step()\n",
        "\n",
        "        # Print the loss\n",
        "        print('Epoch [{}/{}], Step [{}/{}], Loss D: {:.4f}, Loss G: {:.4f}'\n",
        "              .format(epoch+1, 100, i+1, len(train_loader), loss_d.item(), loss_g.item()))\n",
        "        img_list.append(torch.cat([x,y_fake], 1))\n",
        "\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Save the model\n",
        "torch.save(generator.state_dict(), 'generator.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ-f9Kt4tXP_"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(30,30))\n",
        "for i in range(1,16,2):\n",
        "    plt.subplot(4,4,i)\n",
        "    img = np.zeros((224,224,3))\n",
        "    img[:,:,0] = L_df[i]\n",
        "    print(L_df.shape)\n",
        "    plt.title('B&W')\n",
        "    plt.imshow(lab2rgb(img))\n",
        "\n",
        "    plt.subplot(4,4,i+1)\n",
        "    img[:,:,1:] = ab_df[i]\n",
        "    img = img.astype('uint8')\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)\n",
        "    plt.title('Colored')\n",
        "    plt.imshow(img)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}